{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import string, re\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import Binarizer\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filepath, review_type):\n",
    "    \"\"\"This funtion reads in the data, parses it, and separates it into \n",
    "    the appropriate train and test splits. Data is read in in UTF-8, and is\n",
    "    parsed by removing all punctuation. Any review that begins with the \n",
    "    filename 'cv9' is considered to be part of the test set.\n",
    "    \n",
    "    Args:\n",
    "        filepath: path to the corpus\n",
    "        review_type: type of review included in the data within this filepath\n",
    "    Returns:\n",
    "        Returns four separate lists. The test set corpus, the test set labels,\n",
    "        the training set corpus, and the training set labels. \n",
    "    \"\"\"\n",
    "    test, test_labels = [], []\n",
    "    train, train_labels = [], []\n",
    "    \n",
    "    for filename in os.listdir(filepath):\n",
    "        with open(filepath + filename, 'rb') as review:\n",
    "            txt = review.read().decode('utf8', 'surrogateescape')\n",
    "            txt = txt.replace(\"--\", \"\").replace(\"_\", \" \").replace(\"-\", \" \")\n",
    "            translator = str.maketrans('', '', string.punctuation)\n",
    "            txt = txt.translate(translator)\n",
    "            txt = txt.split()\n",
    "            txt = ' '.join(txt)\n",
    "            if filename.startswith('cv9'):\n",
    "                test.append(txt)\n",
    "                test_labels.append(review_type)\n",
    "            else: \n",
    "                train.append(txt)\n",
    "                train_labels.append(review_type)\n",
    "    return(test, test_labels, train, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in both the positive and negative reviews\n",
    "neg_test, neg_test_labels, neg_train, neg_train_labels = read_data('review_polarity.v2/txt_sentoken/neg/', 'Negative')\n",
    "pos_test, pos_test_labels, pos_train, pos_train_labels = read_data('review_polarity.v2/txt_sentoken/pos/', 'Positive')\n",
    "\n",
    "# combine training sets\n",
    "train_labels = neg_train_labels + pos_train_labels\n",
    "train = neg_train + pos_train\n",
    "\n",
    "# combine test sets\n",
    "test_labels = neg_test_labels + pos_test_labels\n",
    "test = neg_test + pos_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_most_informative_features(vectorizer, classifier, n=10):\n",
    "    \"\"\"This function takes a vectorizer and a classifier and prints the\n",
    "    top n most informative features for each class. \n",
    "    \n",
    "    Args:\n",
    "        vectorizer: nltk vectorizer\n",
    "        classifier: nltk classifier\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    class_labels = classifier.classes_\n",
    "    feature_names = vectorizer.get_feature_names()  \n",
    "    topn_pos_class = sorted(zip(classifier.feature_count_[1], feature_names),reverse=True)[:n]\n",
    "    topn_neg_class = sorted(zip(classifier.feature_count_[0], feature_names),reverse=True)[:n]    \n",
    "\n",
    "    print(\"Important words in positive reviews\")\n",
    "    for coef, feature in topn_pos_class:\n",
    "        print(class_labels[1], coef, feature) \n",
    "    print(\"-----------------------------------------\")\n",
    "    print(\"Important words in negative reviews\")\n",
    "    for coef, feature in topn_neg_class:\n",
    "        print(class_labels[0], coef, feature)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adv_adj_only(corpus):\n",
    "    \"\"\"This function takes a movie review and returns only those words\n",
    "    in the review that are adjectives and adverbs.\n",
    "    \n",
    "    Args:\n",
    "        corpus: a preprocessed review\n",
    "    Returns:\n",
    "        Returns a modified review that only includes adjectives and adverbs.\n",
    "    \"\"\"\n",
    "    tags = ['JJ', 'JJR', 'JJS', 'RB', 'RBR', 'RBS']\n",
    "    text = word_tokenize(corpus)\n",
    "    all_tags = pos_tag(text)\n",
    "    result = [word[0] for word in all_tags if word[1] in tags]\n",
    "    result = ' '.join(result)\n",
    "    return(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M1\n",
    "Unigrams, absense/presence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.87\n",
      "Important words in positive reviews\n",
      "Positive 900.0 the\n",
      "Positive 900.0 of\n",
      "Positive 899.0 to\n",
      "Positive 899.0 and\n",
      "Positive 898.0 is\n",
      "-----------------------------------------\n",
      "Important words in negative reviews\n",
      "Negative 899.0 the\n",
      "Negative 899.0 of\n",
      "Negative 899.0 and\n",
      "Negative 898.0 to\n",
      "Negative 898.0 is\n"
     ]
    }
   ],
   "source": [
    "# get word counts\n",
    "vectorizer = CountVectorizer(binary=True)\n",
    "train_features = vectorizer.fit_transform([doc for doc in train])\n",
    "\n",
    "# make naive bayes classifier \n",
    "nb_clf = MultinomialNB()\n",
    "nb_clf.fit(train_features, train_labels)\n",
    "\n",
    "# prep test set\n",
    "test_features = vectorizer.transform([doc for doc in test])\n",
    "\n",
    "# make predictions\n",
    "predictions = nb_clf.predict(test_features)\n",
    "\n",
    "# evaluate\n",
    "accuracy = accuracy_score(predictions, test_labels)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# print most informative words\n",
    "show_most_informative_features(vectorizer, nb_clf, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M2\n",
    "Unigrams with frequency count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.84\n",
      "Important words in positive reviews\n",
      "Positive 37122.0 the\n",
      "Positive 17771.0 and\n",
      "Positive 16692.0 of\n",
      "Positive 14798.0 to\n",
      "Positive 12557.0 is\n",
      "-----------------------------------------\n",
      "Important words in negative reviews\n",
      "Negative 31470.0 the\n",
      "Negative 14011.0 and\n",
      "Negative 13857.0 to\n",
      "Negative 13857.0 of\n",
      "Negative 9961.0 is\n"
     ]
    }
   ],
   "source": [
    "# get word counts\n",
    "vectorizer = CountVectorizer()\n",
    "train_features = vectorizer.fit_transform([doc for doc in train])\n",
    "\n",
    "# make naive bayes classifier \n",
    "nb_clf = MultinomialNB()\n",
    "nb_clf.fit(train_features, train_labels)\n",
    "\n",
    "# prep test set\n",
    "test_features = vectorizer.transform([doc for doc in test])\n",
    "\n",
    "# make predictions\n",
    "predictions = nb_clf.predict(test_features)\n",
    "\n",
    "# evaluate\n",
    "accuracy = accuracy_score(predictions, test_labels)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "show_most_informative_features(vectorizer, nb_clf, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M3\n",
    "Unigrams, only adjectives and adverbs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.85\n",
      "Important words in positive reviews\n",
      "Positive 2666.0 not\n",
      "Positive 1633.0 more\n",
      "Positive 1334.0 so\n",
      "Positive 1247.0 most\n",
      "Positive 1200.0 just\n",
      "-----------------------------------------\n",
      "Important words in negative reviews\n",
      "Negative 2430.0 not\n",
      "Negative 1549.0 so\n",
      "Negative 1391.0 just\n",
      "Negative 1362.0 more\n",
      "Negative 1222.0 even\n"
     ]
    }
   ],
   "source": [
    "# get word counts\n",
    "vectorizer = CountVectorizer()\n",
    "train_features = vectorizer.fit_transform([adv_adj_only(doc) for doc in train])\n",
    "\n",
    "# make naive bayes classifier \n",
    "nb_clf = MultinomialNB()\n",
    "nb_clf.fit(train_features, train_labels)\n",
    "\n",
    "# prep test set\n",
    "test_features = vectorizer.transform([adv_adj_only(doc) for doc in test])\n",
    "\n",
    "# make predictions\n",
    "predictions = nb_clf.predict(test_features)\n",
    "\n",
    "# evaluate\n",
    "accuracy = accuracy_score(predictions, test_labels)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "show_most_informative_features(vectorizer, nb_clf, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M4\n",
    "Unigrams, sublinear TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.85\n",
      "Important words in positive reviews\n",
      "Positive 21.954856077538892 movie\n",
      "Positive 18.705007035879404 like\n",
      "Positive 16.785996727777352 story\n",
      "Positive 16.780804954144276 life\n",
      "Positive 16.69821652018229 just\n",
      "-----------------------------------------\n",
      "Important words in negative reviews\n",
      "Negative 26.6824696024802 movie\n",
      "Negative 21.118291464844972 like\n",
      "Negative 19.702164811842977 just\n",
      "Negative 18.611681285269174 bad\n",
      "Negative 16.958553323083297 good\n"
     ]
    }
   ],
   "source": [
    "# get modified word counts\n",
    "vectorizer = TfidfVectorizer(min_df = 5, max_df = 0.8, stop_words='english', sublinear_tf=True)\n",
    "train_features = vectorizer.fit_transform([doc for doc in train])\n",
    "\n",
    "# make naive bayes classifier \n",
    "nb_clf = MultinomialNB()\n",
    "nb_clf.fit(train_features, train_labels)\n",
    "\n",
    "# prep test set\n",
    "test_features = vectorizer.transform([doc for doc in test])\n",
    "\n",
    "# make predictions\n",
    "predictions = nb_clf.predict(test_features)\n",
    "\n",
    "# evaluate\n",
    "accuracy = accuracy_score(predictions, test_labels)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "show_most_informative_features(vectorizer, nb_clf, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M5\n",
    "Bigrams, absense/presence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.86\n",
      "Important words in positive reviews\n",
      "Positive 845.0 of the\n",
      "Positive 787.0 in the\n",
      "Positive 657.0 to the\n",
      "Positive 649.0 the film\n",
      "Positive 634.0 and the\n",
      "-----------------------------------------\n",
      "Important words in negative reviews\n",
      "Negative 828.0 of the\n",
      "Negative 796.0 in the\n",
      "Negative 624.0 to be\n",
      "Negative 606.0 the film\n",
      "Negative 577.0 to the\n"
     ]
    }
   ],
   "source": [
    "# get absense/presence counts for bigrams\n",
    "vectorizer = CountVectorizer(ngram_range=(2,2), binary=True)\n",
    "train_features = vectorizer.fit_transform([doc for doc in train])\n",
    "\n",
    "# make naive bayes classifier \n",
    "nb_clf = MultinomialNB()\n",
    "nb_clf.fit(train_features, train_labels)\n",
    "\n",
    "# prep test set\n",
    "test_features = vectorizer.transform([doc for doc in test])\n",
    "\n",
    "# make predictions\n",
    "predictions = nb_clf.predict(test_features)\n",
    "\n",
    "# evaluate\n",
    "accuracy = accuracy_score(predictions, test_labels)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "show_most_informative_features(vectorizer, nb_clf, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best performing model is the M1 model (unigram absense/presence)-- this model has <b>87% accuracy</b>. The worst performing model is M2 (unigram frequency counts)-- this model has <b>84% accuracy</b>. Interestingly enough, both of these models have the same top five most influential words, which are all stop words. Binarizing the counts for the M1 model likely removed some of the negative effects of stop words. \n",
    "\n",
    "In general, when looking at the most influential words for all models, each model except M4 is dominated by stop words. For this reason, while the accuracy for M4 is not the best (85%), qualitatively this model appears to be the best because the most influential words appear to capture sentiment more than the other models. Removing stop words and further tuning the parameters used for creating custom stop words would likely help imporove all models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "Using Porter Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_PorterStemmer(review):\n",
    "    ps = PorterStemmer()\n",
    "    words = word_tokenize(review)\n",
    "    stemmed = [ps.stem(word) for word in words]\n",
    "    stemmed_review = ' '.join(stemmed)\n",
    "    return(stemmed_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the train and test corpus\n",
    "stem_train = [my_PorterStemmer(review) for review in train]\n",
    "stem_test = [my_PorterStemmer(review) for review in test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.855\n",
      "Important words in positive reviews\n",
      "Positive 900.0 the\n",
      "Positive 900.0 of\n",
      "Positive 899.0 to\n",
      "Positive 899.0 and\n",
      "Positive 898.0 is\n",
      "-----------------------------------------\n",
      "Important words in negative reviews\n",
      "Negative 899.0 the\n",
      "Negative 899.0 of\n",
      "Negative 899.0 and\n",
      "Negative 898.0 to\n",
      "Negative 898.0 is\n"
     ]
    }
   ],
   "source": [
    "# get word counts\n",
    "vectorizer = CountVectorizer(binary=True)\n",
    "train_features = vectorizer.fit_transform([doc for doc in stem_train])\n",
    "\n",
    "# make naive bayes classifier \n",
    "nb_clf = MultinomialNB()\n",
    "nb_clf.fit(train_features, train_labels)\n",
    "\n",
    "# prep test set\n",
    "test_features = vectorizer.transform([doc for doc in stem_test])\n",
    "\n",
    "# make predictions\n",
    "predictions = nb_clf.predict(test_features)\n",
    "\n",
    "# evaluate\n",
    "accuracy = accuracy_score(predictions, test_labels)\n",
    "print(accuracy)\n",
    "\n",
    "show_most_informative_features(vectorizer, nb_clf, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.84\n",
      "Important words in positive reviews\n",
      "Positive 37122.0 the\n",
      "Positive 17771.0 and\n",
      "Positive 16696.0 of\n",
      "Positive 14799.0 to\n",
      "Positive 12557.0 is\n",
      "-----------------------------------------\n",
      "Important words in negative reviews\n",
      "Negative 31470.0 the\n",
      "Negative 14011.0 and\n",
      "Negative 13858.0 of\n",
      "Negative 13857.0 to\n",
      "Negative 9961.0 is\n"
     ]
    }
   ],
   "source": [
    "# get word counts\n",
    "vectorizer = CountVectorizer()\n",
    "train_features = vectorizer.fit_transform([doc for doc in stem_train])\n",
    "\n",
    "# make naive bayes classifier \n",
    "nb_clf = MultinomialNB()\n",
    "nb_clf.fit(train_features, train_labels)\n",
    "\n",
    "# prep test set\n",
    "test_features = vectorizer.transform([doc for doc in stem_test])\n",
    "\n",
    "# make predictions\n",
    "predictions = nb_clf.predict(test_features)\n",
    "\n",
    "# evaluate\n",
    "accuracy = accuracy_score(predictions, test_labels)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "show_most_informative_features(vectorizer, nb_clf, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.835\n",
      "Important words in positive reviews\n",
      "Positive 2679.0 not\n",
      "Positive 1873.0 hi\n",
      "Positive 1633.0 more\n",
      "Positive 1304.0 so\n",
      "Positive 1248.0 most\n",
      "-----------------------------------------\n",
      "Important words in negative reviews\n",
      "Negative 2434.0 not\n",
      "Negative 1513.0 so\n",
      "Negative 1391.0 just\n",
      "Negative 1361.0 more\n",
      "Negative 1360.0 hi\n"
     ]
    }
   ],
   "source": [
    "# get word counts\n",
    "vectorizer = CountVectorizer()\n",
    "train_features = vectorizer.fit_transform([adv_adj_only(doc) for doc in stem_train])\n",
    "\n",
    "# make naive bayes classifier \n",
    "nb_clf = MultinomialNB()\n",
    "nb_clf.fit(train_features, train_labels)\n",
    "\n",
    "# prep test set\n",
    "test_features = vectorizer.transform([adv_adj_only(doc) for doc in stem_test])\n",
    "\n",
    "# make predictions\n",
    "predictions = nb_clf.predict(test_features)\n",
    "\n",
    "# evaluate\n",
    "accuracy = accuracy_score(predictions, test_labels)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "show_most_informative_features(vectorizer, nb_clf, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.855\n",
      "Important words in positive reviews\n",
      "Positive 22.794618033004646 wa\n",
      "Positive 21.081929361327802 charact\n",
      "Positive 20.976549597038304 like\n",
      "Positive 19.910406575685826 make\n",
      "Positive 19.68015088753407 time\n",
      "-----------------------------------------\n",
      "Important words in negative reviews\n",
      "Negative 24.860276120908456 wa\n",
      "Negative 23.568359090059325 like\n",
      "Negative 21.52631859516604 charact\n",
      "Negative 20.95806019401285 just\n",
      "Negative 19.897462497896765 make\n"
     ]
    }
   ],
   "source": [
    "# get modified word counts\n",
    "vectorizer = TfidfVectorizer(min_df = 5, max_df = 0.8, stop_words='english', sublinear_tf=True)\n",
    "train_features = vectorizer.fit_transform([doc for doc in stem_train])\n",
    "\n",
    "# make naive bayes classifier \n",
    "nb_clf = MultinomialNB()\n",
    "nb_clf.fit(train_features, train_labels)\n",
    "\n",
    "# prep test set\n",
    "test_features = vectorizer.transform([doc for doc in stem_test])\n",
    "\n",
    "# make predictions\n",
    "predictions = nb_clf.predict(test_features)\n",
    "\n",
    "# evaluate\n",
    "accuracy = accuracy_score(predictions, test_labels)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "show_most_informative_features(vectorizer, nb_clf, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.85\n",
      "Important words in positive reviews\n",
      "Positive 845.0 of the\n",
      "Positive 787.0 in the\n",
      "Positive 684.0 the film\n",
      "Positive 657.0 to the\n",
      "Positive 634.0 and the\n",
      "-----------------------------------------\n",
      "Important words in negative reviews\n",
      "Negative 828.0 of the\n",
      "Negative 796.0 in the\n",
      "Negative 644.0 the film\n",
      "Negative 626.0 to be\n",
      "Negative 577.0 to the\n"
     ]
    }
   ],
   "source": [
    "# get absense/presence counts for bigrams\n",
    "vectorizer = CountVectorizer(ngram_range=(2,2), binary=True)\n",
    "train_features = vectorizer.fit_transform([doc for doc in stem_train])\n",
    "\n",
    "# make naive bayes classifier \n",
    "nb_clf = MultinomialNB()\n",
    "nb_clf.fit(train_features, train_labels)\n",
    "\n",
    "# prep test set\n",
    "test_features = vectorizer.transform([doc for doc in stem_test])\n",
    "\n",
    "# make predictions\n",
    "predictions = nb_clf.predict(test_features)\n",
    "\n",
    "# evaluate\n",
    "accuracy = accuracy_score(predictions, test_labels)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "show_most_informative_features(vectorizer, nb_clf, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "After applying the Porter Stemmer to our corpus, the only model that improved was the M4 model, which improved from 85% accuracy to 85.5% accuracy. All other models either stayed constant or dropped in accuracy. Due to such a minor increase on a non-optimal model, and overall a decrease in accuracy for other models, I would argue that it is not worth doing in this scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "NRC_emotion = pd.read_csv(\"NRC_Emotion.txt\", \n",
    "                           sep=\"\\t\", \n",
    "                           skiprows=22, \n",
    "                           names=[\"TargetWord\", \"AffectCategory\", \"AssociationFlag\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words must have an associate to be relevant\n",
    "NRC_emotion = NRC_emotion[NRC_emotion.AssociationFlag == 1]\n",
    "# we only care about positive and negative assocaitions\n",
    "NRC_emotion = NRC_emotion[NRC_emotion.AffectCategory.isin(['negative', 'positive'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TargetWord</th>\n",
       "      <th>AffectCategory</th>\n",
       "      <th>AssociationFlag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>109843</th>\n",
       "      <td>sarcoma</td>\n",
       "      <td>negative</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132073</th>\n",
       "      <td>unattractive</td>\n",
       "      <td>negative</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abandon</td>\n",
       "      <td>negative</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28274</th>\n",
       "      <td>cosmopolitan</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33613</th>\n",
       "      <td>departure</td>\n",
       "      <td>negative</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          TargetWord AffectCategory  AssociationFlag\n",
       "109843       sarcoma       negative                1\n",
       "132073  unattractive       negative                1\n",
       "3            abandon       negative                1\n",
       "28274   cosmopolitan       positive                1\n",
       "33613      departure       negative                1"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NRC_emotion.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_neg_ratio(review, pos_words, neg_words):\n",
    "    \"\"\"This function takes a review and lists of positive and negative \n",
    "    associated words and returns the ratio of positive rewards to negative\n",
    "    words in the review. \n",
    "    \n",
    "    Args:\n",
    "        review: a preprocessed review string\n",
    "        pos_words: a list of positive words\n",
    "        neg_words: a list of negative words\n",
    "    Returns:\n",
    "        Returns a float representing the ratio of positive words in the \n",
    "        review to negative words in the review. \n",
    "    \"\"\"\n",
    "    pos, neg = 0, 0\n",
    "    words = word_tokenize(review)\n",
    "    for word in words: \n",
    "        if word in pos_words:\n",
    "            pos+=1\n",
    "        if word in neg_words:\n",
    "            neg+=1\n",
    "    if neg != 0:\n",
    "        ratio = pos/neg\n",
    "    else:\n",
    "        ratio = pos\n",
    "    return ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the negative and positive words from NRC Lexicon\n",
    "neg = NRC_emotion[NRC_emotion.AffectCategory == 'negative'].TargetWord.values\n",
    "pos = NRC_emotion[NRC_emotion.AffectCategory == 'positive'].TargetWord.values\n",
    "\n",
    "# calculate the ratios for train and test set\n",
    "ratio_train = [pos_neg_ratio(review, pos, neg) for review in train] \n",
    "ratio_test = [pos_neg_ratio(review, pos, neg) for review in test] \n",
    "\n",
    "# make naive bayes classifier \n",
    "nb_clf = MultinomialNB()\n",
    "nb_clf.fit(np.array(ratio_train).reshape(-1, 1), train_labels)\n",
    "\n",
    "# make predictions\n",
    "predictions = nb_clf.predict(np.array(ratio_test).reshape(-1,1))\n",
    "\n",
    "# evaluate\n",
    "accuracy = accuracy_score(predictions, test_labels)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "The NRC Emotion model is the worst performing model that we have seen thus far. This is likely due to the limited vocabulary included in the NRC emotion vocabulary and the fact that the that the vocabulary is not tailored to movie reviews (the topic of our corpus). Additionally, the ratings that we are working with are relatively lengthy and oftentimes describe the movie, not just the person's feelings about the movie. Consequently, some of the adjectives used to describe the movie may confuse the calculated sentiment ratio-- resulting in a poor model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4\n",
    "\n",
    "As discussed throughout this notebook, in order to improve the performance of these models I would begin by removing standard stop words for all model types. Then, I would add to this by removing a list of custom stop words that pertain to this movie reviews corpus that are not helpful when determining sentiment. For example, words such as \"characters\", \"movie\", and \"plot\" are all within the top 100 words used in the training corpus, however, do not contribute to the sentiment analysis. Tuning the parameters used to find these custom stop words would also be an important step in improving the sentiment analysis. \n",
    "\n",
    "Aside from customizing stop words, I would like to experiment with bigram and trigram frequencies in the corpus because I believe phrases like \"see again\" and \"not recommend\" could be very telling in determining sentiment. Lastly, some kind of normalization is likely important for this kind of analysis. Normalization could be done in a variety of ways, such as term frequency normalization (to make sure that imporant words reieve higher weighting), review length normalization (to make sure that the length of the review does not impact the sentiment rating), normalizing by reviewer (some reviewers naturally have a more positive vocabulary than others).  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
